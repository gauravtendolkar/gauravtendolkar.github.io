---
layout: post
title: "4. Generative Adversarial Networks"
posted: "6 June, 2024"
categories: Deep-Generative-Modelling
live: true
---

GANs were first introduced in 2014, less than a year after VAEs, with the paper [*Generative Adversarial Networks](https://arxiv.org/abs/1406.2661).* GAN is not a type of neural network, but a framework for estimating a generative model. In this framework, we train two models, called generator and discriminator, in an adversarial fashion. There is no restriction on the forms the generator and discriminator. If they are parametric and differentiable, we can use gradient descent to train learn the parameters from data. For example, you could have a [VAE as the generator](https://arxiv.org/abs/1512.09300) by modifying the loss function of the VAE appropriately.

## Adversarial Training

Like VAEs, GANs use a latent variable $Z$ with a prior probability distribution $P(z)$. A parametric generator function $G_{\theta}(z)$ deterministically maps the latent sample $z$ to a data sample $x$. Another parametric model $D_{\phi}(x)$, called discriminator, predicts the probability that $x$ came from data instead of being generated by $G_{\theta}(z)$.

The discriminator is fed a mixture of real samples from data and samples generated by $G_{\theta}(z)$ by randomly sampling $z$. Parameters $\phi$ are obtained via MLE. In other words, we find parameters $\phi$ such that $D_{\phi}(x)$ maximizes log likelihood real samples and minimizes log likelihood of samples generated by $G_{\theta}(z)$. That is, we maximize

$$
E_{P^*(x)}\log(D_{\phi}(x)) + E_{P(z)}\log(1-D_{\phi}(G_{\theta}(z)))
$$

We simultaneously find $\theta$ such that discriminator $D_{\phi}(x)$ outputs a high probability for samples generated by $G_{\theta}(z)$. That is, we minimize

$$
E_{P(z)}\log(1-D_{\phi}(G_{\theta}(z)))
$$

With parametric models optimized using mini batch stochastic gradient descent, the algorithm provided by the paper is

---

**Adversarial training algorithm**

---

Initialize generator parameters $\theta$

Initialize discriminator parameters $\phi$

**for** number of training iterations $n$

**for** $k$ steps

sample $m$ samples of latent vector $Z$ as $\{z_1, z_2, ..., z_m\}$

sample $m$ samples of real data $X$ as $\{x_1, x_2, ..., x_m\}$

update discriminator parameters $\phi$ by ascending the gradient

$\nabla_{\phi} ( \ \frac{1}{m}\sum_{i=1}^m \log(D_{\phi}(x_i)) + \log(1-D_{\phi}(G_{\theta}(z_i))) \ )$

**end for**

sample $m$ samples of latent vector $Z$ as $\{z_1, z_2, ..., z_m\}$

update generator parameters $\theta$ by descending the gradient

$\nabla_{\theta} \frac{1}{m}\sum_{i=1}^m \log(1-D_{\phi}(G_{\theta}(z_i)))$

**end for**

---

## Theoretical guarantees

Given infinite capacity generator and discriminator, and a perfect optimization algorithm, adversarial training is theoretically guaranteed to reach the optimum where $G_{\theta}(z)$ becomes equal to true data distribution $P^*(x)$ and discriminator $D_{\phi}(x)$ returns probability of 0.5 for every sample (real or generated). 

In practice, the generator and discriminator is not infinite capacity, we are restricted to mini batch stochastic optimization and we do not let the discriminator reach its optimum for given instance of generator. These simplifications make the adversarial training process unstable.

## Comparison with VAE

In VAE, because of the form of our reconstruction loss, we restricted our output data distribution to be simple distributions. For example, when using sum squared error between original and reconstructed data, we imposed that out output distribution is a mixture of isotropic Gaussians, all of them having the same constant variance. Such restriction can limit the expressivity of the network. Similarly, we also impose constraint on the latent variable distribution.

On the other hand, GANs use an adversarial loss function to train the generator model thereby imposing no restriction on the form of output distribution the generator model can have. Moreover, like VAEs, GANs also learn a simple prior latent distribution but without involving the delicate balancing of reconstruction error vs. regularization. In practice too, GANs often outperform VAEs in terms of quality of generated output.

In the next post, we shall see an implementation of GAN with PyTorch.